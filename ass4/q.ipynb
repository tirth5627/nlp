{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ee5ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read data from '../ass1/gujarati_sentence_tokenized.parquet'...\n",
      "Successfully loaded 100000 sentences for the training set.\n",
      "Tokenizing sentences and adding start/end tokens...\n",
      "Finished preparing 100000 sentences.\n",
      "Building language models...\n",
      "Models built successfully.\n",
      "Vocabulary Size (V): 143826\n",
      "Total tokens in corpus (N): 2008988\n",
      "\n",
      "--- Pre-computing Follower Counts for Token Type Smoothing ---\n",
      "Follower counts computed.\n",
      "\n",
      "--- Selecting 1000 Sentences for Evaluation ---\n",
      "Randomly selected 1000 sentences.\n",
      "\n",
      "--- Applying Smoothed Models to Test Sentences ---\n",
      "Processing sentence 100/1000...\n",
      "Processing sentence 200/1000...\n",
      "Processing sentence 300/1000...\n",
      "Processing sentence 400/1000...\n",
      "Processing sentence 500/1000...\n",
      "Processing sentence 600/1000...\n",
      "Processing sentence 700/1000...\n",
      "Processing sentence 800/1000...\n",
      "Processing sentence 900/1000...\n",
      "Processing sentence 1000/1000...\n",
      "Evaluation complete.\n",
      "\n",
      "--- Sample of Evaluation Results ---\n",
      "                                             Sentence                  Model  Log Probability\n",
      "0   <s> એટલું જ નહીં પરંતુ એર સ્ટ્રાઇક પર વાત કરતા...          unigram_add_1      -208.319981\n",
      "1   <s> એટલું જ નહીં પરંતુ એર સ્ટ્રાઇક પર વાત કરતા...           bigram_add_1      -299.619815\n",
      "2   <s> એટલું જ નહીં પરંતુ એર સ્ટ્રાઇક પર વાત કરતા...         bigram_add_0.5      -295.864589\n",
      "3   <s> એટલું જ નહીં પરંતુ એર સ્ટ્રાઇક પર વાત કરતા...      bigram_token_type      -168.495759\n",
      "4   <s> એટલું જ નહીં પરંતુ એર સ્ટ્રાઇક પર વાત કરતા...          trigram_add_1      -314.176070\n",
      "5   <s> એટલું જ નહીં પરંતુ એર સ્ટ્રાઇક પર વાત કરતા...        trigram_add_0.5      -313.074206\n",
      "6   <s> એટલું જ નહીં પરંતુ એર સ્ટ્રાઇક પર વાત કરતા...     trigram_token_type       -96.693153\n",
      "7   <s> એટલું જ નહીં પરંતુ એર સ્ટ્રાઇક પર વાત કરતા...       quadrigram_add_1      -320.937427\n",
      "8   <s> એટલું જ નહીં પરંતુ એર સ્ટ્રાઇક પર વાત કરતા...     quadrigram_add_0.5      -321.154550\n",
      "9   <s> એટલું જ નહીં પરંતુ એર સ્ટ્રાઇક પર વાત કરતા...  quadrigram_token_type       -77.025338\n",
      "10    <s> જે તેમની 17 વર્ષીય દીકરી પણ જોતી હતી . </s>          unigram_add_1       -69.534714\n",
      "11    <s> જે તેમની 17 વર્ષીય દીકરી પણ જોતી હતી . </s>           bigram_add_1      -108.595765\n",
      "12    <s> જે તેમની 17 વર્ષીય દીકરી પણ જોતી હતી . </s>         bigram_add_0.5      -107.377430\n",
      "13    <s> જે તેમની 17 વર્ષીય દીકરી પણ જોતી હતી . </s>      bigram_token_type       -68.116941\n",
      "14    <s> જે તેમની 17 વર્ષીય દીકરી પણ જોતી હતી . </s>          trigram_add_1      -115.718689\n",
      "15    <s> જે તેમની 17 વર્ષીય દીકરી પણ જોતી હતી . </s>        trigram_add_0.5      -115.427858\n",
      "16    <s> જે તેમની 17 વર્ષીય દીકરી પણ જોતી હતી . </s>     trigram_token_type       -35.821493\n",
      "17    <s> જે તેમની 17 વર્ષીય દીકરી પણ જોતી હતી . </s>       quadrigram_add_1      -118.809304\n",
      "18    <s> જે તેમની 17 વર્ષીય દીકરી પણ જોતી હતી . </s>     quadrigram_add_0.5      -118.853152\n",
      "19    <s> જે તેમની 17 વર્ષીય દીકરી પણ જોતી હતી . </s>  quadrigram_token_type       -38.772840\n",
      "20  <s> પરંતુ ફર્સ્ટ ટાઇમ સેક્સ કરવા જઇ રહ્યા છો ત...          unigram_add_1      -133.111122\n",
      "21  <s> પરંતુ ફર્સ્ટ ટાઇમ સેક્સ કરવા જઇ રહ્યા છો ત...           bigram_add_1      -201.051870\n",
      "\n",
      "Total execution time: 46.23 seconds.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- 1. Data Loading and Preparation ---\n",
    "\n",
    "PARQUET_FILE_PATH = '../ass1/gujarati_sentence_tokenized.parquet'\n",
    "COLUMN_NAME = 'sentence'\n",
    "\n",
    "def load_data_from_parquet(file_path, column_name):\n",
    "    \"\"\"\n",
    "    Loads the full list of sentences from a specified column in a Parquet file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Attempting to read data from '{file_path}'...\")\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        if column_name not in df.columns:\n",
    "            print(f\"Error: Column '{column_name}' not found in the Parquet file.\")\n",
    "            print(f\"Available columns are: {df.columns.tolist()}\")\n",
    "            return None\n",
    "            \n",
    "        # As per the requirement, use the full dataset for training the models.\n",
    "        sentences = df[column_name].tolist()[:100000]\n",
    "        print(f\"Successfully loaded {len(sentences)} sentences for the training set.\")\n",
    "        return sentences\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "        print(\"Please make sure the PARQUET_FILE_PATH is correct.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def gujarati_word_tokenizer(sentence):\n",
    "    \"\"\"\n",
    "    Tokenizes a Gujarati sentence using comprehensive regex logic.\n",
    "    \"\"\"\n",
    "    if not isinstance(sentence, str):\n",
    "        return []\n",
    "        \n",
    "    sentence = re.sub(r'\\s+', ' ', sentence.strip())\n",
    "    \n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    email_pattern = r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b'\n",
    "    date_pattern = r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b|\\b\\d{1,2}(?:st|nd|rd|th)?\\s+\\w+\\s+\\d{4}\\b'\n",
    "    number_pattern = r'\\b\\d+(?:[\\.,]\\d+)?\\b'\n",
    "    full_pattern = re.compile(\n",
    "        f'{url_pattern}|{email_pattern}|{date_pattern}|{number_pattern}|[a-zA-Z]+|[\\u0A80-\\u0AFF]+|[^\\w\\s]',\n",
    "        re.UNICODE\n",
    "    )\n",
    "    words = re.findall(full_pattern, sentence)\n",
    "    return words\n",
    "\n",
    "def prepare_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Tokenizes each sentence and adds start (<s>) and end (</s>) tokens.\n",
    "    \"\"\"\n",
    "    print(\"Tokenizing sentences and adding start/end tokens...\")\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = gujarati_word_tokenizer(sentence)\n",
    "        # We add one start token for unigram/bigram, but it's simpler to be consistent.\n",
    "        # Higher-order models will use multiple start tokens as padding during calculation.\n",
    "        processed_sentences.append(['<s>'] + tokens + ['</s>'])\n",
    "    print(f\"Finished preparing {len(processed_sentences)} sentences.\")\n",
    "    return processed_sentences\n",
    "\n",
    "# --- 2. N-gram Model Building ---\n",
    "\n",
    "def generate_ngrams(words, n):\n",
    "    \"\"\"A generator for n-grams using a sliding window.\"\"\"\n",
    "    for i in range(len(words) - n + 1):\n",
    "        yield tuple(words[i : i + n])\n",
    "\n",
    "def build_ngram_models(sentences):\n",
    "    \"\"\"Builds Unigram, Bigram, Trigram, and Quadrigram models from prepared sentences.\"\"\"\n",
    "    print(\"Building language models...\")\n",
    "    unigram_counts = Counter()\n",
    "    bigram_counts = Counter()\n",
    "    trigram_counts = Counter()\n",
    "    quadrigram_counts = Counter()\n",
    "\n",
    "    total_words = 0\n",
    "    for sentence in sentences:\n",
    "        total_words += len(sentence)\n",
    "        unigram_counts.update(sentence)\n",
    "        bigram_counts.update(generate_ngrams(sentence, 2))\n",
    "        trigram_counts.update(generate_ngrams(sentence, 3))\n",
    "        quadrigram_counts.update(generate_ngrams(sentence, 4))\n",
    "    \n",
    "    print(\"Models built successfully.\")\n",
    "    return {\n",
    "        'unigram': unigram_counts,\n",
    "        'bigram': bigram_counts,\n",
    "        'trigram': trigram_counts,\n",
    "        'quadrigram': quadrigram_counts\n",
    "    }, total_words\n",
    "\n",
    "# --- 3. Smoothing Implementations ---\n",
    "\n",
    "# 3a. Add-K Smoothing Probability Functions\n",
    "def prob_add_k_unigram(word, k, vocab_size, unigram_counts, total_word_count):\n",
    "    \"\"\"Calculates P(word) with Add-K smoothing.\"\"\"\n",
    "    numerator = unigram_counts.get(word, 0) + k\n",
    "    denominator = total_word_count + (k * vocab_size)\n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def prob_add_k_bigram(word, prev_word, k, vocab_size, unigram_counts, bigram_counts):\n",
    "    \"\"\"Calculates P(word | prev_word) with Add-K smoothing.\"\"\"\n",
    "    numerator = bigram_counts.get((prev_word, word), 0) + k\n",
    "    denominator = unigram_counts.get(prev_word, 0) + (k * vocab_size)\n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def prob_add_k_trigram(word, p1, p2, k, vocab_size, bigram_counts, trigram_counts):\n",
    "    \"\"\"Calculates P(word | p1, p2) with Add-K smoothing.\"\"\"\n",
    "    context = (p1, p2)\n",
    "    numerator = trigram_counts.get((*context, word), 0) + k\n",
    "    denominator = bigram_counts.get(context, 0) + (k * vocab_size)\n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def prob_add_k_quadrigram(word, p1, p2, p3, k, vocab_size, trigram_counts, quadrigram_counts):\n",
    "    \"\"\"Calculates P(word | p1, p2, p3) with Add-K smoothing.\"\"\"\n",
    "    context = (p1, p2, p3)\n",
    "    numerator = quadrigram_counts.get((*context, word), 0) + k\n",
    "    denominator = trigram_counts.get(context, 0) + (k * vocab_size)\n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "# 3b. Token Type Smoothing Preparation and Probability Functions\n",
    "def compute_follower_counts(ngram_counts):\n",
    "    \"\"\"Computes the number of unique token types that follow a given context.\"\"\"\n",
    "    follower_counts = defaultdict(set)\n",
    "    for ngram in ngram_counts:\n",
    "        context = ngram[:-1]\n",
    "        follower = ngram[-1]\n",
    "        follower_counts[context].add(follower)\n",
    "    return {context: len(followers) for context, followers in follower_counts.items()}\n",
    "\n",
    "def prob_token_type_bigram(word, prev_word, k, unigram_counts, bigram_counts, follower_counts):\n",
    "    \"\"\"Calculates P(word | prev_word) with Token Type Smoothing.\"\"\"\n",
    "    context = (prev_word,)\n",
    "    num_follower_types = follower_counts.get(context, 0)\n",
    "    numerator = bigram_counts.get((prev_word, word), 0) + k\n",
    "    denominator = unigram_counts.get(prev_word, 0) + (k * num_follower_types)\n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def prob_token_type_trigram(word, p1, p2, k, bigram_counts, trigram_counts, follower_counts):\n",
    "    \"\"\"Calculates P(word | p1, p2) with Token Type Smoothing.\"\"\"\n",
    "    context = (p1, p2)\n",
    "    num_follower_types = follower_counts.get(context, 0)\n",
    "    numerator = trigram_counts.get((*context, word), 0) + k\n",
    "    denominator = bigram_counts.get(context, 0) + (k * num_follower_types)\n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def prob_token_type_quadrigram(word, p1, p2, p3, k, trigram_counts, quadrigram_counts, follower_counts):\n",
    "    \"\"\"Calculates P(word | p1, p2, p3) with Token Type Smoothing.\"\"\"\n",
    "    context = (p1, p2, p3)\n",
    "    num_follower_types = follower_counts.get(context, 0)\n",
    "    numerator = quadrigram_counts.get((*context, word), 0) + k\n",
    "    denominator = trigram_counts.get(context, 0) + (k * num_follower_types)\n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "\n",
    "# --- 4. Sentence Probability Calculation ---\n",
    "\n",
    "def calculate_sentence_log_prob(sentence, model_name, smoothing_type, k, params):\n",
    "    \"\"\"Calculates the log probability of a sentence using a specified model and smoothing.\"\"\"\n",
    "    log_prob = 0.0\n",
    "    epsilon = 1e-10 \n",
    "\n",
    "    # Unpack parameters\n",
    "    counts, total_words, vocab_size, follower_maps = params\n",
    "    unigrams, bigrams, trigrams, quadrigrams = counts['unigram'], counts['bigram'], counts['trigram'], counts['quadrigram']\n",
    "    \n",
    "    # Pad sentence with start tokens for higher-order models\n",
    "    padded_sentence = ['<s>'] * 3 + sentence[1:] # Max padding needed is 3 for quadrigram\n",
    "\n",
    "    for i in range(1, len(sentence)): # Start from first word after <s>\n",
    "        word = sentence[i]\n",
    "        prob = 0.0\n",
    "        \n",
    "        # Select the correct function based on model and smoothing\n",
    "        if model_name == 'unigram':\n",
    "             prob = prob_add_k_unigram(word, k, vocab_size, unigrams, total_words)\n",
    "        elif model_name == 'bigram':\n",
    "            prev = padded_sentence[i+2] # Context: w_{i-1}\n",
    "            if smoothing_type == 'add_k':\n",
    "                prob = prob_add_k_bigram(word, prev, k, vocab_size, unigrams, bigrams)\n",
    "            elif smoothing_type == 'token_type':\n",
    "                prob = prob_token_type_bigram(word, prev, k, unigrams, bigrams, follower_maps['bigram'])\n",
    "        elif model_name == 'trigram':\n",
    "            p1, p2 = padded_sentence[i+1], padded_sentence[i+2] # Context: w_{i-2}, w_{i-1}\n",
    "            if smoothing_type == 'add_k':\n",
    "                prob = prob_add_k_trigram(word, p1, p2, k, vocab_size, bigrams, trigrams)\n",
    "            elif smoothing_type == 'token_type':\n",
    "                prob = prob_token_type_trigram(word, p1, p2, k, bigrams, trigrams, follower_maps['trigram'])\n",
    "        elif model_name == 'quadrigram':\n",
    "            p1, p2, p3 = padded_sentence[i], padded_sentence[i+1], padded_sentence[i+2] # Context: w_{i-3}, w_{i-2}, w_{i-1}\n",
    "            if smoothing_type == 'add_k':\n",
    "                prob = prob_add_k_quadrigram(word, p1, p2, p3, k, vocab_size, trigrams, quadrigrams)\n",
    "            elif smoothing_type == 'token_type':\n",
    "                prob = prob_token_type_quadrigram(word, p1, p2, p3, k, trigrams, quadrigrams, follower_maps['quadrigram'])\n",
    "\n",
    "        log_prob += np.log(prob + epsilon)\n",
    "        \n",
    "    return log_prob\n",
    "\n",
    "# --- 5. Main Execution Block ---\n",
    "def main():\n",
    "    \"\"\"Main function to run the entire pipeline.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Load and Prepare Data\n",
    "    all_sentences = load_data_from_parquet(PARQUET_FILE_PATH, COLUMN_NAME)\n",
    "    if not all_sentences:\n",
    "        print(\"\\nExecution stopped: No data was loaded.\")\n",
    "        return\n",
    "\n",
    "    prepared_sentences = prepare_sentences(all_sentences)\n",
    "    \n",
    "    # 2. Build Language Models on the full dataset\n",
    "    counts, total_word_count = build_ngram_models(prepared_sentences)\n",
    "    vocab_size = len(counts['unigram'])\n",
    "    print(f\"Vocabulary Size (V): {vocab_size}\")\n",
    "    print(f\"Total tokens in corpus (N): {total_word_count}\")\n",
    "\n",
    "    # 3. Pre-compute follower counts for Token Type Smoothing\n",
    "    print(\"\\n--- Pre-computing Follower Counts for Token Type Smoothing ---\")\n",
    "    follower_maps = {\n",
    "        'bigram': compute_follower_counts(counts['bigram']),\n",
    "        'trigram': compute_follower_counts(counts['trigram']),\n",
    "        'quadrigram': compute_follower_counts(counts['quadrigram'])\n",
    "    }\n",
    "    print(\"Follower counts computed.\")\n",
    "    \n",
    "    # 4. Select 1000 random sentences for evaluation\n",
    "    print(\"\\n--- Selecting 1000 Sentences for Evaluation ---\")\n",
    "    if len(prepared_sentences) >= 1000:\n",
    "        test_sentences = random.sample(prepared_sentences, 1000)\n",
    "        print(f\"Randomly selected {len(test_sentences)} sentences.\")\n",
    "    else:\n",
    "        test_sentences = prepared_sentences\n",
    "        print(f\"Warning: Fewer than 1000 sentences. Using all {len(test_sentences)} for testing.\")\n",
    "\n",
    "    # 5. Apply models and compute probabilities\n",
    "    print(\"\\n--- Applying Smoothed Models to Test Sentences ---\")\n",
    "    results = []\n",
    "    models_to_test = ['unigram', 'bigram', 'trigram', 'quadrigram']\n",
    "    k_for_add_k = 0.5  # A common choice for K in Add-K smoothing\n",
    "\n",
    "    # Package all model parameters for easy passing\n",
    "    params = (counts, total_word_count, vocab_size, follower_maps)\n",
    "\n",
    "    for i, sentence in enumerate(test_sentences):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processing sentence {i+1}/{len(test_sentences)}...\")\n",
    "\n",
    "        sent_str = ' '.join(sentence)\n",
    "        if len(sent_str) > 70: sent_str = sent_str[:67] + '...'\n",
    "\n",
    "        for model in models_to_test:\n",
    "            # Unigram only uses Add-K\n",
    "            if model == 'unigram':\n",
    "                log_prob_add1 = calculate_sentence_log_prob(sentence, model, 'add_k', 1, params)\n",
    "                results.append({'Sentence': sent_str, 'Model': f'{model}_add_1', 'Log Probability': log_prob_add1})\n",
    "                continue\n",
    "\n",
    "            # Add-1 Smoothing (Laplace)\n",
    "            log_prob_add1 = calculate_sentence_log_prob(sentence, model, 'add_k', 1, params)\n",
    "            results.append({'Sentence': sent_str, 'Model': f'{model}_add_1', 'Log Probability': log_prob_add1})\n",
    "\n",
    "            # Add-K Smoothing\n",
    "            log_prob_add_k = calculate_sentence_log_prob(sentence, model, 'add_k', k_for_add_k, params)\n",
    "            results.append({'Sentence': sent_str, 'Model': f'{model}_add_{k_for_add_k}', 'Log Probability': log_prob_add_k})\n",
    "            \n",
    "            # Token Type Smoothing (k=1 is a common choice)\n",
    "            log_prob_token_type = calculate_sentence_log_prob(sentence, model, 'token_type', 1, params)\n",
    "            results.append({'Sentence': sent_str, 'Model': f'{model}_token_type', 'Log Probability': log_prob_token_type})\n",
    "            \n",
    "    print(\"Evaluation complete.\")\n",
    "\n",
    "    # 6. Display Results\n",
    "    print(\"\\n--- Sample of Evaluation Results ---\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "    pd.set_option('display.max_rows', 24)\n",
    "    pd.set_option('display.width', 120)\n",
    "    print(results_df.head(22)) # Display results for a couple of sentences\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
