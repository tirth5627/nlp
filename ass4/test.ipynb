{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1edf3b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "PARQUET_FILE_PATH = '../ass1/gujarati_sentence_tokenized.parquet'\n",
    "COLUMN_NAME = 'sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26e9fca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read data from '../ass1/gujarati_sentence_tokenized.parquet'...\n",
      "Successfully loaded 1000000 sentences.\n"
     ]
    }
   ],
   "source": [
    "def load_tokenized_data_from_parquet(file_path, column_name):\n",
    "    \"\"\"\n",
    "    Loads a list of tokenized sentences from a specified column in a Parquet file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Attempting to read data from '{file_path}'...\")\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        if column_name not in df.columns:\n",
    "            print(f\"Error: Column '{column_name}' not found in the Parquet file.\")\n",
    "            print(f\"Available columns are: {df.columns.tolist()}\")\n",
    "            return None\n",
    "            \n",
    "        tokenized_sentences = df[column_name].tolist()[:1000000]\n",
    "        print(f\"Successfully loaded {len(tokenized_sentences)} sentences.\")\n",
    "        return tokenized_sentences\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "        print(\"Please make sure the PARQUET_FILE_PATH is correct.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the data\n",
    "tokenized_data = load_tokenized_data_from_parquet(PARQUET_FILE_PATH, COLUMN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b2fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gujarati_word_tokenizer(sentence):\n",
    "    \"\"\"\n",
    "    Tokenizes a Gujarati sentence using the provided regex logic.\n",
    "    \"\"\"\n",
    "    # Ensure the input is a string, as data from files can sometimes be other types\n",
    "    if not isinstance(sentence, str):\n",
    "        return []\n",
    "        \n",
    "    sentence = re.sub(r'\\s+', ' ', sentence.strip())\n",
    "    \n",
    "    # Using the provided regex patterns\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    email_pattern = r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b'\n",
    "    date_pattern = r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b|\\b\\d{1,2}(?:st|nd|rd|th)?\\s+\\w+\\s+\\d{4}\\b'\n",
    "    number_pattern = r'\\b\\d+(?:[\\.,]\\d+)?\\b'\n",
    "    # Pattern to capture Gujarati script, Latin script, numbers, URLs, emails, dates, and punctuation\n",
    "    full_pattern = re.compile(\n",
    "        f'{url_pattern}|{email_pattern}|{date_pattern}|{number_pattern}|[a-zA-Z]+|[\\u0A80-\\u0AFF]+|[^\\w\\s]',\n",
    "        re.UNICODE\n",
    "    )\n",
    "    words = re.findall(full_pattern, sentence)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06077232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_end_tokens(sentences):\n",
    "    \"\"\"\n",
    "    Adds start (<s>) and end (</s>) tokens to each sentence.\n",
    "    Handles cases where sentences are strings by splitting them into tokens.\n",
    "    \"\"\"\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Check if the item is a string (and not already a list of tokens)\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = gujarati_word_tokenizer(sentence)\n",
    "            processed_sentences.append(['<s>'] + tokens + ['</s>'])\n",
    "        elif isinstance(sentence, list):\n",
    "            # If it's already a list, just add the markers\n",
    "            processed_sentences.append(['<s>'] + sentence + ['</s>'])\n",
    "    return processed_sentences\n",
    "\n",
    "def pad_sentences(sentences, n):\n",
    "    \"\"\"Pads sentences with n-1 start tokens for an n-gram model.\"\"\"\n",
    "    padded_sentences = []\n",
    "    for sentence in sentences:\n",
    "        padding = ['<s>'] * (n - 1)\n",
    "        padded_sentence = padding + sentence[1:]\n",
    "        padded_sentences.append(padded_sentence)\n",
    "    return padded_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f834d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams_manual(words, n):\n",
    "    \"\"\"A manual generator for n-grams using a sliding window.\"\"\"\n",
    "    for i in range(len(words) - n + 1):\n",
    "        yield tuple(words[i : i + n])\n",
    "\n",
    "def build_ngram_models_manual(sentences):\n",
    "    \"\"\"Builds Unigram, Bigram, Trigram, and Quadrigram models.\"\"\"\n",
    "    print(\"\\nBuilding language models...\")\n",
    "    unigram_counts = Counter()\n",
    "    bigram_counts = Counter()\n",
    "    trigram_counts = Counter()\n",
    "    quadrigram_counts = Counter()\n",
    "\n",
    "    all_words = [word for sentence in sentences for word in sentence]\n",
    "    unigram_counts.update(all_words)\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        bigram_counts.update(generate_ngrams_manual(sentence, 2))\n",
    "        trigram_counts.update(generate_ngrams_manual(sentence, 3))\n",
    "        quadrigram_counts.update(generate_ngrams_manual(sentence, 4))\n",
    "    \n",
    "    print(\"Models built successfully.\")\n",
    "    return unigram_counts, bigram_counts, trigram_counts, quadrigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0803e1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsmoothed probability functions are defined.\n"
     ]
    }
   ],
   "source": [
    "def calculate_bigram_prob(word, prev_word, unigram_counts, bigram_counts):\n",
    "    \"\"\"Calculates P(word | prev_word)\"\"\"\n",
    "    bigram_count = bigram_counts.get((prev_word, word), 0)\n",
    "    prev_word_count = unigram_counts.get(prev_word, 0)\n",
    "    if prev_word_count == 0: return 0.0\n",
    "    return bigram_count / prev_word_count\n",
    "\n",
    "def calculate_trigram_prob(word, prev_word1, prev_word2, bigram_counts, trigram_counts):\n",
    "    \"\"\"Calculates P(word | prev_word1, prev_word2)\"\"\"\n",
    "    trigram_count = trigram_counts.get((prev_word1, prev_word2, word), 0)\n",
    "    bigram_context_count = bigram_counts.get((prev_word1, prev_word2), 0)\n",
    "    if bigram_context_count == 0: return 0.0\n",
    "    return trigram_count / bigram_context_count\n",
    "\n",
    "def calculate_quadrigram_prob(word, p1, p2, p3, trigram_counts, quadrigram_counts):\n",
    "    \"\"\"Calculates P(word | p1, p2, p3)\"\"\"\n",
    "    quadrigram_count = quadrigram_counts.get((p1, p2, p3, word), 0)\n",
    "    trigram_context_count = trigram_counts.get((p1, p2, p3), 0)\n",
    "    if trigram_context_count == 0: return 0.0\n",
    "    return quadrigram_count / trigram_context_count\n",
    "\n",
    "print(\"Unsmoothed probability functions are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84124a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothed probability functions are defined.\n"
     ]
    }
   ],
   "source": [
    "def calculate_add_k_bigram_prob(word, prev_word, k, vocab_size, unigram_counts, bigram_counts):\n",
    "    \"\"\"Calculates P(word | prev_word) with Add-K smoothing.\"\"\"\n",
    "    numerator = bigram_counts.get((prev_word, word), 0) + k\n",
    "    denominator = unigram_counts.get(prev_word, 0) + (k * vocab_size)\n",
    "    if denominator == 0: return 0.0\n",
    "    return numerator / denominator\n",
    "\n",
    "def calculate_add_k_trigram_prob(word, p1, p2, k, vocab_size, bigram_counts, trigram_counts):\n",
    "    \"\"\"Calculates P(word | p1, p2) with Add-K smoothing.\"\"\"\n",
    "    numerator = trigram_counts.get((p1, p2, word), 0) + k\n",
    "    denominator = bigram_counts.get((p1, p2), 0) + (k * vocab_size)\n",
    "    if denominator == 0: return 0.0\n",
    "    return numerator / denominator\n",
    "\n",
    "def calculate_add_k_quadrigram_prob(word, p1, p2, p3, k, vocab_size, trigram_counts, quadrigram_counts):\n",
    "    \"\"\"Calculates P(word | p1, p2, p3) with Add-K smoothing.\"\"\"\n",
    "    numerator = quadrigram_counts.get((p1, p2, p3, word), 0) + k\n",
    "    denominator = trigram_counts.get((p1, p2, p3), 0) + (k * vocab_size)\n",
    "    if denominator == 0: return 0.0\n",
    "    return numerator / denominator\n",
    "\n",
    "print(\"Smoothed probability functions are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc0b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and splitting data...\n",
      "  Training set size: 98000\n",
      "  Validation set size: 1000\n",
      "  Test set size: 1000\n",
      "Building language models on the training set...\n",
      "Models built successfully.\n",
      "Vocabulary Size (V): 142817\n",
      "Initializing Good-Turing for 1-grams...\n",
      "Initializing Good-Turing for 2-grams...\n",
      "\n",
      "--- Good-Turing Frequency Table for Unigram (n=1) ---\n",
      "Vocab size (V): 142817\n",
      "n-gram order (n): 1\n",
      "Observed (seen) types: 142817 (OK)\n",
      "Total observed tokens (N): 1951974\n",
      "N1 (types with count==1): 84806\n",
      "Total prob mass for unseen types (P_unseen_total): 4.344628e-02\n",
      "For C=0: Nc0 = 0; total unseen probability mass = 4.344628e-02\n",
      "\n",
      " C    Nc        C*  CumSeen\n",
      " 1 84806  0.441643    84806\n",
      " 2 18727  1.391146   103533\n",
      " 3  8684  2.405804   112217\n",
      " 4  5223  3.298870   117440\n",
      " 5  3446  4.572258   120886\n",
      " 6  2626  5.421935   123512\n",
      " 7  2034  6.383481   125546\n",
      " 8  1623  7.264325   127169\n",
      " 9  1310  8.496183   128479\n",
      "10  1113  9.013477   129592\n",
      "11   912 10.210526   130504\n",
      "12   776 12.346649   131280\n",
      "13   737 12.233379   132017\n",
      "14   644 12.763975   132661\n",
      "15   548 14.306569   133209\n",
      "16   490 14.397959   133699\n",
      "17   415 15.701205   134114\n",
      "18   362 18.475138   134476\n",
      "19   352 18.806818   134828\n",
      "20   331 19.794562   135159\n",
      "21   312 18.262821   135471\n",
      "22   259 22.200772   135730\n",
      "23   250 22.560000   135980\n",
      "24   235 22.340426   136215\n",
      "25   210 22.038095   136425\n",
      "26   178 25.028090   136603\n",
      "27   165 26.812121   136768\n",
      "28   158 30.468354   136926\n",
      "29   166 26.204819   137092\n",
      "30   145 33.565517   137237\n",
      "31   157 29.146497   137394\n",
      "32   143 30.461538   137537\n",
      "33   132 29.621212   137669\n",
      "34   115 40.782609   137784\n",
      "35   134 27.134328   137918\n",
      "36   101 35.534653   138019\n",
      "37    97 55.628866   138116\n",
      "38   142 32.957746   138258\n",
      "39   120 29.333333   138378\n",
      "40    88 46.590909   138466\n",
      "41   100 36.960000   138566\n",
      "42    88 46.909091   138654\n",
      "43    96 36.666667   138750\n",
      "44    80 40.500000   138830\n",
      "45    72 51.750000   138902\n",
      "46    81 41.777778   138983\n",
      "47    72 33.333333   139055\n",
      "48    50 60.760000   139105\n",
      "49    62 52.419355   139167\n",
      "50    65 47.861538   139232\n",
      "\n",
      "Top non-zero Nc (count -> Nc):\n",
      "  Count   1 -> Nc = 84806\n",
      "  Count   2 -> Nc = 18727\n",
      "  Count   3 -> Nc = 8684\n",
      "  Count   4 -> Nc = 5223\n",
      "  Count   5 -> Nc = 3446\n",
      "  Count   6 -> Nc = 2626\n",
      "  Count   7 -> Nc = 2034\n",
      "  Count   8 -> Nc = 1623\n",
      "  Count   9 -> Nc = 1310\n",
      "  Count  10 -> Nc = 1113\n",
      "  Count  11 -> Nc = 912\n",
      "  Count  12 -> Nc = 776\n",
      "  Count  13 -> Nc = 737\n",
      "  Count  14 -> Nc = 644\n",
      "  Count  15 -> Nc = 548\n",
      "  Count  16 -> Nc = 490\n",
      "  Count  17 -> Nc = 415\n",
      "  Count  18 -> Nc = 362\n",
      "  Count  19 -> Nc = 352\n",
      "  Count  20 -> Nc = 331\n",
      "\n",
      "--- Good-Turing Frequency Table for Bigram (n=2) ---\n",
      "Vocab size (V): 142817\n",
      "n-gram order (n): 2\n",
      "Observed (seen) types: 889526 (OK)\n",
      "Total observed tokens (N): 1853974\n",
      "N1 (types with count==1): 743851\n",
      "Total prob mass for unseen types (P_unseen_total): 4.012198e-01\n",
      "For C=0: Nc0 = infeasible (V^2 = 20,396,695,489 > 10,000,000); total unseen probability mass = 4.012198e-01\n",
      "\n",
      " C     Nc        C*  CumSeen\n",
      " 1 743851  0.196935   743851\n",
      " 2  73245  1.041737   817096\n",
      " 3  25434  2.010537   842530\n",
      " 4  12784  2.997888   855314\n",
      " 5   7665  3.913894   862979\n",
      " 6   5000  4.912600   867979\n",
      " 7   3509  5.986891   871488\n",
      " 8   2626  6.741432   874114\n",
      " 9   1967  7.702084   876081\n",
      "10   1515  8.858086   877596\n",
      "11   1220 10.298361   878816\n",
      "12   1047 11.112703   879863\n",
      "13    895 10.808939   880758\n",
      "14    691 13.654124   881449\n",
      "15    629 13.659777   882078\n",
      "16    537 15.607076   882615\n",
      "17    493 14.860041   883108\n",
      "18    407 16.805897   883515\n",
      "19    360 19.111111   883875\n",
      "20    344 16.970930   884219\n",
      "21    278 24.136691   884497\n",
      "22    305 18.626230   884802\n",
      "23    247 21.376518   885049\n",
      "24    220 21.590909   885269\n",
      "25    190 27.368421   885459\n",
      "26    200 25.245000   885659\n",
      "27    187 22.909091   885846\n",
      "28    153 28.431373   885999\n",
      "29    150 24.000000   886149\n",
      "30    120 28.416667   886269\n",
      "31    110 27.927273   886379\n",
      "32     96 37.468750   886475\n",
      "33    109 28.697248   886584\n",
      "34     92 34.239130   886676\n",
      "35     90 33.200000   886766\n",
      "36     83 34.771084   886849\n",
      "37     78 41.410256   886927\n",
      "38     85 35.788235   887012\n",
      "39     78 33.846154   887090\n",
      "40     66 44.106061   887156\n",
      "41     71 42.000000   887227\n",
      "42     71 39.971831   887298\n",
      "43     66 40.000000   887364\n",
      "44     60 45.000000   887424\n",
      "45     60 31.433333   887484\n",
      "46     41 66.487805   887525\n",
      "47     58 42.206897   887583\n",
      "48     51 42.274510   887634\n",
      "49     44 42.045455   887678\n",
      "50     37 57.891892   887715\n",
      "\n",
      "Top non-zero Nc (count -> Nc):\n",
      "  Count   1 -> Nc = 743851\n",
      "  Count   2 -> Nc = 73245\n",
      "  Count   3 -> Nc = 25434\n",
      "  Count   4 -> Nc = 12784\n",
      "  Count   5 -> Nc = 7665\n",
      "  Count   6 -> Nc = 5000\n",
      "  Count   7 -> Nc = 3509\n",
      "  Count   8 -> Nc = 2626\n",
      "  Count   9 -> Nc = 1967\n",
      "  Count  10 -> Nc = 1515\n",
      "  Count  11 -> Nc = 1220\n",
      "  Count  12 -> Nc = 1047\n",
      "  Count  13 -> Nc = 895\n",
      "  Count  14 -> Nc = 691\n",
      "  Count  15 -> Nc = 629\n",
      "  Count  16 -> Nc = 537\n",
      "  Count  17 -> Nc = 493\n",
      "  Count  18 -> Nc = 407\n",
      "  Count  19 -> Nc = 360\n",
      "  Count  20 -> Nc = 344\n",
      "\n",
      "Total Log Probability of Bigram GT on Validation Set: -261347.29\n",
      "Total Log Probability of Bigram GT on Test Set: -245811.09\n",
      "\n",
      "--- Finding best lambdas for Deleted Interpolation ---\n",
      "Trying lambdas 0.1, 0.1, 0.1, 0.7 -> LogProb: -155133.39\n",
      "Trying lambdas 0.1, 0.1, 0.3, 0.5 -> LogProb: -151859.88\n",
      "Trying lambdas 0.1, 0.1, 0.5, 0.3 -> LogProb: -152531.50\n",
      "Trying lambdas 0.1, 0.1, 0.7, 0.1 -> LogProb: -158897.22\n",
      "Trying lambdas 0.1, 0.3, 0.1, 0.5 -> LogProb: -155639.34\n",
      "Trying lambdas 0.1, 0.3, 0.3, 0.3 -> LogProb: -154222.97\n",
      "Trying lambdas 0.1, 0.3, 0.5, 0.1 -> LogProb: -159914.87\n",
      "Trying lambdas 0.1, 0.5, 0.1, 0.3 -> LogProb: -158946.93\n",
      "Trying lambdas 0.1, 0.5, 0.3, 0.1 -> LogProb: -162282.24\n",
      "Trying lambdas 0.1, 0.7, 0.1, 0.1 -> LogProb: -167918.58\n",
      "Trying lambdas 0.3, 0.1, 0.1, 0.5 -> LogProb: -157552.85\n",
      "Trying lambdas 0.3, 0.1, 0.3, 0.3 -> LogProb: -155680.10\n",
      "Trying lambdas 0.3, 0.1, 0.5, 0.1 -> LogProb: -161135.70\n",
      "Trying lambdas 0.3, 0.3, 0.1, 0.3 -> LogProb: -159976.54\n",
      "Trying lambdas 0.3, 0.3, 0.3, 0.1 -> LogProb: -163136.55\n",
      "Trying lambdas 0.3, 0.5, 0.1, 0.1 -> LogProb: -168624.17\n",
      "Trying lambdas 0.5, 0.1, 0.1, 0.3 -> LogProb: -162046.35\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "# --- 0. Global Settings ---\n",
    "PARQUET_FILE_PATH = '../ass1/gujarati_sentence_tokenized.parquet'\n",
    "COLUMN_NAME = 'sentence'\n",
    "\n",
    "# --- 1. Data Loading and Splitting ---\n",
    "\n",
    "def gujarati_word_tokenizer(sentence):\n",
    "    \"\"\"Tokenizes a Gujarati sentence using comprehensive regex logic.\"\"\"\n",
    "    if not isinstance(sentence, str): return []\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence.strip())\n",
    "    # (Regex patterns are omitted for brevity but should be included)\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    email_pattern = r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b'\n",
    "    # ... other patterns\n",
    "    number_pattern = r'\\b\\d+(?:[\\.,]\\d+)?\\b'\n",
    "    full_pattern = re.compile(\n",
    "        f'{url_pattern}|{email_pattern}|[a-zA-Z]+|[\\u0A80-\\u0AFF]+|[^\\w\\s]', re.UNICODE\n",
    "    )\n",
    "    return re.findall(full_pattern, sentence)\n",
    "\n",
    "def load_and_split_data(file_path, column_name):\n",
    "    \"\"\"Loads data, shuffles it, and splits into train, validation, and test sets.\"\"\"\n",
    "    print(\"Loading and splitting data...\")\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        # Shuffle the entire dataframe. frac=1 means 100%. random_state for reproducibility.\n",
    "        df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)[:100000]\n",
    "\n",
    "        # Define split sizes\n",
    "        val_size = 1000\n",
    "        test_size = 1000\n",
    "\n",
    "        # Create the splits\n",
    "        val_set = df_shuffled.iloc[0:val_size][column_name].tolist()\n",
    "        test_set = df_shuffled.iloc[val_size : val_size + test_size][column_name].tolist()\n",
    "        train_set = df_shuffled.iloc[val_size + test_size :][column_name].tolist()\n",
    "\n",
    "        print(f\"  Training set size: {len(train_set)}\")\n",
    "        print(f\"  Validation set size: {len(val_set)}\")\n",
    "        print(f\"  Test set size: {len(test_set)}\")\n",
    "\n",
    "        return train_set, val_set, test_set\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data loading/splitting: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def prepare_sentences(sentence_list):\n",
    "    \"\"\"Tokenizes and adds boundary markers to a list of sentences.\"\"\"\n",
    "    return [['<s>'] + gujarati_word_tokenizer(s) + ['</s>'] for s in sentence_list]\n",
    "\n",
    "\n",
    "# --- 2. N-gram Model Building (Unchanged) ---\n",
    "def generate_ngrams(words, n):\n",
    "    for i in range(len(words) - n + 1):\n",
    "        yield tuple(words[i:i + n])\n",
    "\n",
    "def build_ngram_models(sentences):\n",
    "    \"\"\"Builds all n-gram models from a list of sentences.\"\"\"\n",
    "    print(\"Building language models on the training set...\")\n",
    "    counts = {\n",
    "        'unigram': Counter(), 'bigram': Counter(),\n",
    "        'trigram': Counter(), 'quadrigram': Counter()\n",
    "    }\n",
    "    for sentence in sentences:\n",
    "        counts['unigram'].update(sentence)\n",
    "        counts['bigram'].update(generate_ngrams(sentence, 2))\n",
    "        counts['trigram'].update(generate_ngrams(sentence, 3))\n",
    "        counts['quadrigram'].update(generate_ngrams(sentence, 4))\n",
    "    print(\"Models built successfully.\")\n",
    "    return counts\n",
    "\n",
    "# --- 3. Good-Turing Smoothing Implementation ---\n",
    "\n",
    "class GoodTuringSmoother:\n",
    "    \"\"\"A class to handle Good-Turing smoothing for a given n-gram model.\"\"\"\n",
    "    def __init__(self, ngram_counts, vocab_size, n):\n",
    "        self.ngram_counts = ngram_counts\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n = n\n",
    "        \n",
    "        print(f\"Initializing Good-Turing for {n}-grams...\")\n",
    "        \n",
    "        # Total number of observed n-gram tokens\n",
    "        self.N = sum(self.ngram_counts.values())\n",
    "        \n",
    "        # Calculate frequency of frequencies (Nc)\n",
    "        freq_of_freqs = Counter(self.ngram_counts.values())\n",
    "        self.N_c = defaultdict(int, freq_of_freqs)\n",
    "        \n",
    "        # N1 is the number of n-grams that appeared exactly once\n",
    "        self.N1 = self.N_c[1]\n",
    "        \n",
    "        # Total probability mass for all unseen n-grams\n",
    "        self.P_unseen_total = self.N1 / self.N if self.N > 0 else 0\n",
    "        \n",
    "        # Estimate the number of unseen n-gram types. THIS IS THE CHALLENGE.\n",
    "        num_seen_types = len(self.ngram_counts)\n",
    "        # For unigrams, this is feasible\n",
    "        if n == 1:\n",
    "            num_unseen_types = self.vocab_size - num_seen_types\n",
    "        else:\n",
    "            # For n > 1, V^n is too large. We can't calculate this.\n",
    "            # We'll just use the total probability mass when an unseen n-gram is queried.\n",
    "            num_unseen_types = -1 # Sentinel for \"infeasible to calculate\"\n",
    "\n",
    "        if num_unseen_types > 0:\n",
    "            self.P_unseen_individual = self.P_unseen_total / num_unseen_types\n",
    "        else:\n",
    "            # A fallback: distribute the mass over a very large number as a heuristic\n",
    "            # This is not theoretically pure but avoids division by zero.\n",
    "            self.P_unseen_individual = self.P_unseen_total / (self.vocab_size**n * 0.001) if self.vocab_size > 0 else 0\n",
    "\n",
    "\n",
    "    def get_adjusted_count(self, c):\n",
    "        \"\"\"Calculates c* = (c+1) * N(c+1) / N(c)\"\"\"\n",
    "        if self.N_c[c] == 0:\n",
    "            return c # Fallback if Nc is 0\n",
    "        return (c + 1) * self.N_c[c + 1] / self.N_c[c]\n",
    "\n",
    "    def get_probability(self, ngram):\n",
    "        \"\"\"Returns the smoothed probability of a given n-gram.\"\"\"\n",
    "        c = self.ngram_counts.get(ngram, 0)\n",
    "        \n",
    "        if c == 0:\n",
    "            # The n-gram is unseen\n",
    "            return self.P_unseen_individual\n",
    "        else:\n",
    "            # The n-gram is seen, use the adjusted count c*\n",
    "            # If N(c+1) is 0, c* will be 0. Fall back to MLE in that case to avoid 0 prob.\n",
    "            adjusted_c = self.get_adjusted_count(c)\n",
    "            if adjusted_c == 0:\n",
    "                # Fallback for the highest frequency items\n",
    "                return c / self.N\n",
    "            return adjusted_c / self.N\n",
    "\n",
    "    def calculate_sentence_prob(self, sentence):\n",
    "        \"\"\"Calculates the log probability of a sentence using this smoother.\"\"\"\n",
    "        log_prob = 0.0\n",
    "        epsilon = 1e-12\n",
    "        \n",
    "        # Pad with start tokens for context\n",
    "        padded_sentence = ['<s>'] * (self.n) + sentence[1:]\n",
    "        \n",
    "        for i in range(1, len(sentence)):\n",
    "            # Create the n-gram ending at the current word\n",
    "            context_start = i + self.n - self.n\n",
    "            ngram = tuple(padded_sentence[context_start : context_start + self.n])\n",
    "            prob = self.get_probability(ngram)\n",
    "            log_prob += np.log(prob + epsilon)\n",
    "            \n",
    "        return log_prob\n",
    "\n",
    "def display_good_turing_table(smoother, model_name, max_c=50, infeasible_threshold=1e7):\n",
    "    \"\"\"\n",
    "    Enhanced display for Good-Turing: prints Nc, adjusted counts, cumulative seen,\n",
    "    and an estimate (or infeasible note) of unseen types for n-grams.\n",
    "    \n",
    "    Args:\n",
    "        smoother: GoodTuringSmoother instance\n",
    "        model_name: string label for printing\n",
    "        max_c: max count C to show rows for (1..max_c)\n",
    "        infeasible_threshold: if V^n > this, we mark unseen as infeasible\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Good-Turing Frequency Table for {model_name} (n={smoother.n}) ---\")\n",
    "    \n",
    "    # Basic stats\n",
    "    num_seen_types = len(smoother.ngram_counts)          # number of observed n-gram types (C>0)\n",
    "    total_observed_tokens = smoother.N                  # sum of counts\n",
    "    N1 = smoother.N1                                    # number of types with count == 1\n",
    "    P_unseen_total = smoother.P_unseen_total            # total prob mass assigned to unseen types\n",
    "    \n",
    "    # Try to compute Nc0 (number of unseen types)\n",
    "    V = smoother.vocab_size\n",
    "    n = smoother.n\n",
    "    est_unseen_types = None\n",
    "    nc0_str = None\n",
    "    if n == 1:\n",
    "        # exact\n",
    "        nc0 = max(0, V - num_seen_types)\n",
    "        est_unseen_types = nc0\n",
    "        nc0_str = str(nc0)\n",
    "    else:\n",
    "        # estimate if V^n is small enough\n",
    "        try:\n",
    "            possible_types = V ** n\n",
    "            if possible_types <= infeasible_threshold:\n",
    "                nc0 = max(0, int(possible_types) - num_seen_types)\n",
    "                est_unseen_types = nc0\n",
    "                nc0_str = str(nc0)\n",
    "            else:\n",
    "                nc0_str = f\"infeasible (V^{n} = {possible_types:,} > {int(infeasible_threshold):,})\"\n",
    "                est_unseen_types = None\n",
    "        except OverflowError:\n",
    "            nc0_str = \"infeasible (overflow)\"\n",
    "            est_unseen_types = None\n",
    "\n",
    "    # Build rows for C = 1..max_c\n",
    "    rows = []\n",
    "    cum_seen = 0\n",
    "    for c in range(1, max_c + 1):\n",
    "        Nc = smoother.N_c.get(c, 0)\n",
    "        # adjusted c* if Nc>0 (safe fallback inside smoother.get_adjusted_count)\n",
    "        c_star = smoother.get_adjusted_count(c) if Nc > 0 else 0.0\n",
    "        cum_seen += Nc\n",
    "        rows.append({\n",
    "            'C': c,\n",
    "            'Nc': Nc,\n",
    "            'C*': f\"{c_star:.6f}\",\n",
    "            'CumSeen': cum_seen\n",
    "        })\n",
    "    \n",
    "    # Summaries\n",
    "    total_Nc_seen = sum(smoother.N_c[c] for c in smoother.N_c if c >= 1)\n",
    "    # sanity check: total_Nc_seen should equal num_seen_types\n",
    "    sanity = \"(OK)\" if total_Nc_seen == num_seen_types else f\"(mismatch: sum Nc={total_Nc_seen} != seen={num_seen_types})\"\n",
    "\n",
    "    # For C=0 per-type adjusted estimate (if computable)\n",
    "    if est_unseen_types is not None and est_unseen_types > 0:\n",
    "        per_unseen_prob = P_unseen_total / est_unseen_types if est_unseen_types > 0 else 0.0\n",
    "        c0_display = f\"Nc0 = {est_unseen_types}, per-type prob â‰ˆ {per_unseen_prob:.4e}\"\n",
    "    else:\n",
    "        c0_display = f\"Nc0 = {nc0_str}; total unseen probability mass = {P_unseen_total:.6e}\"\n",
    "\n",
    "    # Print summary block\n",
    "    print(f\"Vocab size (V): {V}\")\n",
    "    print(f\"n-gram order (n): {n}\")\n",
    "    print(f\"Observed (seen) types: {num_seen_types} {sanity}\")\n",
    "    print(f\"Total observed tokens (N): {total_observed_tokens}\")\n",
    "    print(f\"N1 (types with count==1): {N1}\")\n",
    "    print(f\"Total prob mass for unseen types (P_unseen_total): {P_unseen_total:.6e}\")\n",
    "    print(f\"For C=0: {c0_display}\")\n",
    "    print(\"\")\n",
    "\n",
    "    # Print the detailed table as a pandas DataFrame for readability\n",
    "    df = pd.DataFrame(rows)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.width', 200)\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "    # Also print the top few Nc entries (if many zeros)\n",
    "    nonzero_Nc = {c: smoother.N_c[c] for c in sorted(smoother.N_c) if smoother.N_c[c] > 0}\n",
    "    top_items = list(nonzero_Nc.items())[:20]\n",
    "    if top_items:\n",
    "        print(\"\\nTop non-zero Nc (count -> Nc):\")\n",
    "        for c, nc in top_items:\n",
    "            print(f\"  Count {c:3d} -> Nc = {nc}\")\n",
    "\n",
    "\n",
    "# --- 4. Deleted Interpolation Implementation ---\n",
    "\n",
    "def find_best_lambdas(val_sentences, counts, vocab_size):\n",
    "    \"\"\"Finds the best lambda values for a quadrigram model using the validation set.\"\"\"\n",
    "    print(\"\\n--- Finding best lambdas for Deleted Interpolation ---\")\n",
    "    \n",
    "    best_lambdas = {}\n",
    "    best_log_prob = -np.inf\n",
    "    \n",
    "    # Simple grid search over possible lambda values\n",
    "    for l4 in np.arange(0.1, 1.0, 0.2):\n",
    "        for l3 in np.arange(0.1, 1.0 - l4, 0.2):\n",
    "            for l2 in np.arange(0.1, 1.0 - l4 - l3, 0.2):\n",
    "                l1 = 1.0 - l4 - l3 - l2\n",
    "                if l1 < 0: continue\n",
    "                \n",
    "                lambdas = {'l4': l4, 'l3': l3, 'l2': l2, 'l1': l1}\n",
    "                current_log_prob = 0\n",
    "                \n",
    "                for sentence in val_sentences:\n",
    "                    current_log_prob += calculate_interpolated_sentence_prob(sentence, lambdas, counts)\n",
    "                \n",
    "                print(f\"Trying lambdas {l4:.1f}, {l3:.1f}, {l2:.1f}, {l1:.1f} -> LogProb: {current_log_prob:.2f}\")\n",
    "                \n",
    "                if current_log_prob > best_log_prob:\n",
    "                    best_log_prob = current_log_prob\n",
    "                    best_lambdas = lambdas\n",
    "                    \n",
    "    print(f\"\\nBest lambdas found: {best_lambdas} with log probability {best_log_prob:.2f}\")\n",
    "    return best_lambdas\n",
    "\n",
    "def calculate_interpolated_sentence_prob(sentence, lambdas, counts):\n",
    "    \"\"\"Calculates sentence probability using an interpolated quadrigram model.\"\"\"\n",
    "    log_prob = 0.0\n",
    "    epsilon = 1e-12\n",
    "    \n",
    "    unigrams, bigrams, trigrams, quadrigrams = counts['unigram'], counts['bigram'], counts['trigram'], counts['quadrigram']\n",
    "    \n",
    "    padded_sentence = ['<s>'] * 3 + sentence\n",
    "    \n",
    "    for i in range(3, len(padded_sentence)):\n",
    "        w_i = padded_sentence[i]\n",
    "        c4 = tuple(padded_sentence[i-3:i])\n",
    "        c3 = tuple(padded_sentence[i-2:i])\n",
    "        c2 = tuple(padded_sentence[i-1:i])\n",
    "        c1 = tuple(padded_sentence[i-1:i-0]) # should be just w_{i-1} for bigram context\n",
    "\n",
    "        p4 = quadrigrams.get((*c4, w_i), 0) / trigrams.get(c4, 1)\n",
    "        p3 = trigrams.get((*c3, w_i), 0) / bigrams.get(c3, 1)\n",
    "        p2 = bigrams.get((*c2, w_i), 0) / unigrams.get(c2[0], 1)\n",
    "        p1 = unigrams.get(w_i, 0) / sum(unigrams.values())\n",
    "        \n",
    "        prob = (lambdas['l4'] * p4) + (lambdas['l3'] * p3) + \\\n",
    "               (lambdas['l2'] * p2) + (lambdas['l1'] * p1)\n",
    "               \n",
    "        log_prob += np.log(prob + epsilon)\n",
    "        \n",
    "    return log_prob\n",
    "\n",
    "# --- 5. Main Execution Block ---\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Load and split the data\n",
    "    train_data, val_data, test_data = load_and_split_data(PARQUET_FILE_PATH, COLUMN_NAME)\n",
    "    if train_data is None: return\n",
    "    \n",
    "    # 2. Prepare (tokenize) the datasets\n",
    "    train_sents = prepare_sentences(train_data)\n",
    "    val_sents = prepare_sentences(val_data)\n",
    "    test_sents = prepare_sentences(test_data)\n",
    "    \n",
    "    # 3. Build n-gram models on the TRAINING set\n",
    "    counts = build_ngram_models(train_sents)\n",
    "    vocab_size = len(counts['unigram'])\n",
    "    print(f\"Vocabulary Size (V): {vocab_size}\")\n",
    "    \n",
    "    # 4. Implement Good-Turing Smoothing\n",
    "    gt_unigram = GoodTuringSmoother(counts['unigram'], vocab_size, 1)\n",
    "    gt_bigram = GoodTuringSmoother(counts['bigram'], vocab_size, 2)\n",
    "    \n",
    "    # 5. Display Good-Turing Tables\n",
    "    display_good_turing_table(gt_unigram, \"Unigram\")\n",
    "    display_good_turing_table(gt_bigram, \"Bigram\")\n",
    "\n",
    "    # 6. Evaluate Good-Turing on validation and test sets\n",
    "    val_prob_gt_bigram = sum(gt_bigram.calculate_sentence_prob(s) for s in val_sents)\n",
    "    test_prob_gt_bigram = sum(gt_bigram.calculate_sentence_prob(s) for s in test_sents)\n",
    "    print(f\"\\nTotal Log Probability of Bigram GT on Validation Set: {val_prob_gt_bigram:.2f}\")\n",
    "    print(f\"Total Log Probability of Bigram GT on Test Set: {test_prob_gt_bigram:.2f}\")\n",
    "    \n",
    "    # 7. Implement and evaluate Deleted Interpolation\n",
    "    best_lambdas = find_best_lambdas(val_sents, counts, vocab_size)\n",
    "    \n",
    "    # 8. Evaluate the final interpolated model on the TEST set\n",
    "    test_prob_interp = sum(calculate_interpolated_sentence_prob(s, best_lambdas, counts) for s in test_sents)\n",
    "    print(f\"\\nTotal Log Probability of Interpolated Quadrigram on Test Set: {test_prob_interp:.2f}\")\n",
    "\n",
    "    print(f\"\\nTotal execution time: {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
