{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4589de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "TOKEN_FILE = r\"F:/collage/NLP/Assignment_1/2/gu_meta_part_1.txt\"  # file containing one sentence per line\n",
    "BATCH_SIZE = 700_000    # number of tokens to read per batch\n",
    "MODEL_DIR = \"ngram_model\"\n",
    "CHECKPOINT_FILE = os.path.join(MODEL_DIR, \"checkpoint.pkl\")\n",
    "# ----------------------------\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ---------- Data Loader ----------\n",
    "def sentence_batches(file_path, batch_size, start_pos=0):\n",
    "    \"\"\"\n",
    "    Yield batches of tokens from sentences with start/end markers.\n",
    "    Each line in the file = one sentence.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        # Skip already processed lines if resuming\n",
    "        for _ in range(start_pos):\n",
    "            next(f, None)\n",
    "\n",
    "        batch = []\n",
    "        pos = start_pos\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                tokens = line.split()  # basic whitespace tokenization\n",
    "                tokens = [\"<s>\"] + tokens + [\"</s>\"]  # add sentence boundary markers\n",
    "                batch.extend(tokens)\n",
    "                pos += 1\n",
    "                if len(batch) >= batch_size:\n",
    "                    yield batch, pos\n",
    "                    batch = []\n",
    "        if batch:\n",
    "            yield batch, pos\n",
    "\n",
    "\n",
    "# ---------- Checkpoint Helpers ----------\n",
    "def save_checkpoint(state):\n",
    "    with open(CHECKPOINT_FILE, \"wb\") as f:\n",
    "        pickle.dump(state, f)\n",
    "\n",
    "\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "\n",
    "# ---------- Training ----------\n",
    "def train_ngram_model(ngram_size: int):\n",
    "    \"\"\"\n",
    "    Train an n-gram model of given size with checkpoint support.\n",
    "    Saves:\n",
    "        - final_ngram_counts.pkl\n",
    "        - final_context_counts.pkl\n",
    "        - final_ngram_probs.pkl\n",
    "    \"\"\"\n",
    "    # Try to resume from checkpoint\n",
    "    checkpoint = load_checkpoint()\n",
    "    if checkpoint and checkpoint[\"ngram_size\"] == ngram_size:\n",
    "        print(f\"[Resuming] {ngram_size}-gram training from checkpoint at sentence {checkpoint['pos']}\")\n",
    "        ngram_counts = checkpoint[\"ngram_counts\"]\n",
    "        context_counts = checkpoint[\"context_counts\"]\n",
    "        total_tokens = checkpoint[\"total_tokens\"]\n",
    "        start_pos = checkpoint[\"pos\"]\n",
    "        batch_no = checkpoint[\"batch_no\"]\n",
    "    else:\n",
    "        print(f\"[Starting Fresh] {ngram_size}-gram training\")\n",
    "        ngram_counts = defaultdict(int)\n",
    "        context_counts = defaultdict(int)\n",
    "        total_tokens = 0\n",
    "        start_pos = 0\n",
    "        batch_no = 0\n",
    "\n",
    "    for batch, pos in sentence_batches(TOKEN_FILE, BATCH_SIZE, start_pos):\n",
    "        for i in range(len(batch) - ngram_size + 1):\n",
    "            ngram = tuple(batch[i:i + ngram_size])\n",
    "            context = ngram[:-1] if ngram_size > 1 else ()\n",
    "            ngram_counts[ngram] += 1\n",
    "            context_counts[context] += 1\n",
    "\n",
    "        batch_no += 1\n",
    "        print(f\"[Batch {batch_no}] Processed up to sentence {pos}\")\n",
    "\n",
    "        # For unigrams: maintain total token count\n",
    "        if ngram_size == 1:\n",
    "            total_tokens += len(batch)\n",
    "            context_counts[()] = total_tokens\n",
    "\n",
    "        # Save checkpoint after each batch\n",
    "        save_checkpoint({\n",
    "            \"ngram_size\": ngram_size,\n",
    "            \"ngram_counts\": ngram_counts,\n",
    "            \"context_counts\": context_counts,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"pos\": pos,\n",
    "            \"batch_no\": batch_no\n",
    "        })\n",
    "    print(f\"[Checkpoint Saved] {ngram_size}-gram at batch {batch_no}, sentence {pos:,}\")\n",
    "\n",
    "    # Save final counts\n",
    "    with open(os.path.join(MODEL_DIR, f\"final_{ngram_size}gram_counts.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(dict(ngram_counts), f)\n",
    "    with open(os.path.join(MODEL_DIR, f\"final_{ngram_size}gram_context_counts.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(dict(context_counts), f)\n",
    "\n",
    "    print(f\"[Training Complete] Final {ngram_size}-gram counts saved.\")\n",
    "\n",
    "    # ---- Compute probabilities ----\n",
    "    ngram_probs = {}\n",
    "    for ngram, count in ngram_counts.items():\n",
    "        context = ngram[:-1] if ngram_size > 1 else ()\n",
    "        context_count = context_counts[context]\n",
    "        if context_count > 0:\n",
    "            ngram_probs[ngram] = count / context_count\n",
    "\n",
    "    # Save probabilities\n",
    "    with open(os.path.join(MODEL_DIR, f\"final_{ngram_size}gram_probs.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(ngram_probs, f)\n",
    "\n",
    "    print(f\"[Probabilities Saved] {len(ngram_probs)} entries stored.\")\n",
    "\n",
    "    # Remove checkpoint after successful completion\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        os.remove(CHECKPOINT_FILE)\n",
    "\n",
    "    return ngram_counts, context_counts, ngram_probs\n",
    "\n",
    "\n",
    "# ---------- Loader ----------\n",
    "def load_ngram_model(ngram_size: int):\n",
    "    \"\"\"Load counts and probabilities for a trained n-gram model.\"\"\"\n",
    "    with open(os.path.join(MODEL_DIR, f\"final_{ngram_size}gram_counts.pkl\"), \"rb\") as f:\n",
    "        ngram_counts = defaultdict(int, pickle.load(f))\n",
    "    with open(os.path.join(MODEL_DIR, f\"final_{ngram_size}gram_context_counts.pkl\"), \"rb\") as f:\n",
    "        context_counts = defaultdict(int, pickle.load(f))\n",
    "    with open(os.path.join(MODEL_DIR, f\"final_{ngram_size}gram_probs.pkl\"), \"rb\") as f:\n",
    "        ngram_probs = pickle.load(f)\n",
    "    return ngram_counts, context_counts, ngram_probs\n",
    "\n",
    "\n",
    "# ---------- Example Run ----------\n",
    "if _name_ == \"_main_\":\n",
    "    for n in [1, 2, 3, 4]:  # unigram, bigram, trigram, 4-gram\n",
    "        print(f\"\\n--- Training {n}-gram model ---\")\n",
    "        counts, contexts, probs = train_ngram_model(n)\n",
    "        print(f\"Model size ({n}-gram): {len(counts)} ngrams, {len(probs)} probabilities\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
