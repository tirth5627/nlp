{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4589de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading parquet from: ../ass1/gujarati_sentence_tokenized.parquet\n",
      "Loaded 12118231 sentences.\n",
      "Tokenizing and preparing sentences (adding <s> x3 and </s>) ...\n"
     ]
    }
   ],
   "source": [
    "# language_models_full.py\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# -------------------------\n",
    "# Configuration\n",
    "# -------------------------\n",
    "PARQUET_FILE_PATH = '../ass1/gujarati_sentence_tokenized.parquet'  # update if needed\n",
    "COLUMN_NAME = 'sentence'\n",
    "RANDOM_SEED = 42\n",
    "DEBUG_LIMIT = None   # set to an int for quick debugging (e.g. 5000). Set to None to use full file.\n",
    "RESULTS_CSV = 'lm_evaluation_results.csv'\n",
    "\n",
    "# -------------------------\n",
    "# Tokenizer / Preprocessing\n",
    "# -------------------------\n",
    "def gujarati_word_tokenizer(sentence):\n",
    "    \"\"\"\n",
    "    Tokenizes a Gujarati sentence using comprehensive regex logic.\n",
    "    Returns list of token strings.\n",
    "    \"\"\"\n",
    "    if not isinstance(sentence, str):\n",
    "        return []\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence.strip())\n",
    "\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    email_pattern = r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b'\n",
    "    date_pattern = r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b|\\b\\d{1,2}(?:st|nd|rd|th)?\\s+\\w+\\s+\\d{4}\\b'\n",
    "    number_pattern = r'\\b\\d+(?:[\\.,]\\d+)?\\b'\n",
    "    # match english words, gujarati unicode range, or single non-whitespace punctuation\n",
    "    full_pattern = re.compile(\n",
    "        f'{url_pattern}|{email_pattern}|{date_pattern}|{number_pattern}|[a-zA-Z]+|[\\u0A80-\\u0AFF]+|[^\\w\\s]',\n",
    "        re.UNICODE\n",
    "    )\n",
    "    words = re.findall(full_pattern, sentence)\n",
    "    return words\n",
    "\n",
    "def load_data_from_parquet(file_path, column_name, debug_limit=None):\n",
    "    \"\"\"\n",
    "    Loads sentences from Parquet file. Uses full dataset unless debug_limit provided.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Reading parquet from: {file_path}\")\n",
    "        df = pd.read_parquet(file_path)\n",
    "        if column_name not in df.columns:\n",
    "            raise KeyError(f\"Column '{column_name}' not found. Available: {df.columns.tolist()}\")\n",
    "        sentences = df[column_name].astype(str).tolist()\n",
    "        if debug_limit:\n",
    "            sentences = sentences[:debug_limit]\n",
    "            print(f\"DEBUG: limiting to first {debug_limit} sentences.\")\n",
    "        print(f\"Loaded {len(sentences)} sentences.\")\n",
    "        return sentences\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "def prepare_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Tokenize and add start tokens for higher-order ngrams.\n",
    "    We'll add 3 start tokens ('<s>') so quadrigram contexts are easy,\n",
    "    and one end token '</s>'.\n",
    "    \"\"\"\n",
    "    print(\"Tokenizing and preparing sentences (adding <s> x3 and </s>) ...\")\n",
    "    processed = []\n",
    "    for sent in sentences:\n",
    "        tokens = gujarati_word_tokenizer(sent)\n",
    "        # Add 3 start tokens for consistent context across all n-gram orders\n",
    "        processed.append(['<s>','<s>','<s>'] + tokens + ['</s>'])\n",
    "    print(f\"Prepared {len(processed)} sentences.\")\n",
    "    return processed\n",
    "\n",
    "# -------------------------\n",
    "# N-gram builders\n",
    "# -------------------------\n",
    "def generate_ngrams(tokens, n):\n",
    "    \"\"\"\n",
    "    Yields n-gram tuples from a list of tokens.\n",
    "    \"\"\"\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        yield tuple(tokens[i:i+n])\n",
    "\n",
    "def build_ngram_models(prepared_sentences):\n",
    "    \"\"\"\n",
    "    Build unigram/bigram/trigram/quadrigram counts and return along with total token count.\n",
    "    \"\"\"\n",
    "    print(\"Building n-gram counts on full dataset...\")\n",
    "    unigram_counts = Counter()\n",
    "    bigram_counts = Counter()\n",
    "    trigram_counts = Counter()\n",
    "    quadrigram_counts = Counter()\n",
    "    total_tokens = 0\n",
    "\n",
    "    for sent in prepared_sentences:\n",
    "        total_tokens += len(sent)\n",
    "        unigram_counts.update(sent)\n",
    "        bigram_counts.update(generate_ngrams(sent, 2))\n",
    "        trigram_counts.update(generate_ngrams(sent, 3))\n",
    "        quadrigram_counts.update(generate_ngrams(sent, 4))\n",
    "\n",
    "    print(\"Done building n-gram counts.\")\n",
    "    return {\n",
    "        'unigram': unigram_counts,\n",
    "        'bigram': bigram_counts,\n",
    "        'trigram': trigram_counts,\n",
    "        'quadrigram': quadrigram_counts\n",
    "    }, total_tokens\n",
    "\n",
    "# -------------------------\n",
    "# Smoothing probability functions\n",
    "# -------------------------\n",
    "def prob_add_k_unigram(word, k, vocab_size, unigram_counts, total_tokens):\n",
    "    numerator = unigram_counts.get(word, 0) + k\n",
    "    denominator = total_tokens + k * vocab_size\n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def prob_add_k_bigram(word, prev_word, k, vocab_size, unigram_counts, bigram_counts):\n",
    "    numerator = bigram_counts.get((prev_word, word), 0) + k\n",
    "    denominator = unigram_counts.get(prev_word, 0) + k * vocab_size\n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def prob_add_k_trigram(word, p1, p2, k, vocab_size, bigram_counts, trigram_counts):\n",
    "    context = (p1, p2)\n",
    "    numerator = trigram_counts.get((*context, word), 0) + k\n",
    "    denominator = bigram_counts.get(context, 0) + k * vocab_size\n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def prob_add_k_quadrigram(word, p1, p2, p3, k, vocab_size, trigram_counts, quadrigram_counts):\n",
    "    context = (p1, p2, p3)\n",
    "    numerator = quadrigram_counts.get((*context, word), 0) + k\n",
    "    denominator = trigram_counts.get(context, 0) + k * vocab_size\n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "# Token-Type smoothing utilities\n",
    "def compute_follower_counts(ngram_counts):\n",
    "    \"\"\"\n",
    "    ngram_counts: Counter where keys are tuples (of length n), value counts\n",
    "    returns: dict mapping context tuple (n-1 length) -> number of unique follower types\n",
    "    \"\"\"\n",
    "    follower_map = defaultdict(set)\n",
    "    for ngram in ngram_counts:\n",
    "        context = ngram[:-1]\n",
    "        follower = ngram[-1]\n",
    "        follower_map[context].add(follower)\n",
    "    # convert to lengths\n",
    "    return {context: len(fset) for context, fset in follower_map.items()}\n",
    "\n",
    "def prob_token_type_bigram(word, prev_word, k, unigram_counts, bigram_counts, follower_counts, fallback_vocab_size):\n",
    "    context = (prev_word,)\n",
    "    num_follower_types = follower_counts.get(context, 0)\n",
    "    # fallback to vocab size if follower_types is 0 (avoid zero denominator scaling)\n",
    "    scale = num_follower_types if num_follower_types > 0 else fallback_vocab_size\n",
    "    numerator = bigram_counts.get((prev_word, word), 0) + k\n",
    "    denominator = unigram_counts.get(prev_word, 0) + k * scale\n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def prob_token_type_trigram(word, p1, p2, k, bigram_counts, trigram_counts, follower_counts, fallback_vocab_size):\n",
    "    context = (p1, p2)\n",
    "    num_follower_types = follower_counts.get(context, 0)\n",
    "    scale = num_follower_types if num_follower_types > 0 else fallback_vocab_size\n",
    "    numerator = trigram_counts.get((*context, word), 0) + k\n",
    "    denominator = bigram_counts.get(context, 0) + k * scale\n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "def prob_token_type_quadrigram(word, p1, p2, p3, k, trigram_counts, quadrigram_counts, follower_counts, fallback_vocab_size):\n",
    "    context = (p1, p2, p3)\n",
    "    num_follower_types = follower_counts.get(context, 0)\n",
    "    scale = num_follower_types if num_follower_types > 0 else fallback_vocab_size\n",
    "    numerator = quadrigram_counts.get((*context, word), 0) + k\n",
    "    denominator = trigram_counts.get(context, 0) + k * scale\n",
    "    return numerator / denominator if denominator > 0 else 0.0\n",
    "\n",
    "# -------------------------\n",
    "# Sentence log-prob calculation\n",
    "# -------------------------\n",
    "def calculate_sentence_log_prob(sentence_tokens, model_name, smoothing_type, k, params):\n",
    "    \"\"\"\n",
    "    sentence_tokens: tokenized sentence already prepared with <s> x3 and </s>\n",
    "    model_name: 'unigram'|'bigram'|'trigram'|'quadrigram'\n",
    "    smoothing_type: 'add_k'|'token_type'\n",
    "    k: smoothing parameter\n",
    "    params: tuple (counts, total_tokens, vocab_size, follower_maps)\n",
    "    returns: log probability (natural log)\n",
    "    \"\"\"\n",
    "    counts, total_tokens, vocab_size, follower_maps = params\n",
    "    unigrams = counts['unigram']\n",
    "    bigrams = counts['bigram']\n",
    "    trigrams = counts['trigram']\n",
    "    quadrigrams = counts['quadrigram']\n",
    "\n",
    "    log_prob = 0.0\n",
    "    eps = 1e-12  # to avoid log(0); we already aim to avoid zero probs by smoothing\n",
    "\n",
    "    # iterate over tokens starting after the 3 start tokens (index 3) up to end\n",
    "    # We include the end token '</s>' in scoring.\n",
    "    for i in range(3, len(sentence_tokens)):\n",
    "        w = sentence_tokens[i]\n",
    "        prob = 0.0\n",
    "        if model_name == 'unigram':\n",
    "            prob = prob_add_k_unigram(w, k, vocab_size, unigrams, total_tokens)\n",
    "        elif model_name == 'bigram':\n",
    "            prev = sentence_tokens[i-1]\n",
    "            if smoothing_type == 'add_k':\n",
    "                prob = prob_add_k_bigram(w, prev, k, vocab_size, unigrams, bigrams)\n",
    "            elif smoothing_type == 'token_type':\n",
    "                prob = prob_token_type_bigram(w, prev, k, unigrams, bigrams, follower_maps['bigram'], vocab_size)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown smoothing for bigram\")\n",
    "        elif model_name == 'trigram':\n",
    "            p1 = sentence_tokens[i-2]\n",
    "            p2 = sentence_tokens[i-1]\n",
    "            if smoothing_type == 'add_k':\n",
    "                prob = prob_add_k_trigram(w, p1, p2, k, vocab_size, bigrams, trigrams)\n",
    "            elif smoothing_type == 'token_type':\n",
    "                prob = prob_token_type_trigram(w, p1, p2, k, bigrams, trigrams, follower_maps['trigram'], vocab_size)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown smoothing for trigram\")\n",
    "        elif model_name == 'quadrigram':\n",
    "            p1 = sentence_tokens[i-3]\n",
    "            p2 = sentence_tokens[i-2]\n",
    "            p3 = sentence_tokens[i-1]\n",
    "            if smoothing_type == 'add_k':\n",
    "                prob = prob_add_k_quadrigram(w, p1, p2, p3, k, vocab_size, trigrams, quadrigrams)\n",
    "            elif smoothing_type == 'token_type':\n",
    "                prob = prob_token_type_quadrigram(w, p1, p2, p3, k, trigrams, quadrigrams, follower_maps['quadrigram'], vocab_size)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown smoothing for quadrigram\")\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model name\")\n",
    "\n",
    "        # avoid -inf\n",
    "        log_prob += math.log(prob + eps)\n",
    "\n",
    "    return log_prob\n",
    "\n",
    "# -------------------------\n",
    "# Main pipeline\n",
    "# -------------------------\n",
    "def main():\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Load data (full unless DEBUG_LIMIT set)\n",
    "    sentences_raw = load_data_from_parquet(PARQUET_FILE_PATH, COLUMN_NAME, debug_limit=DEBUG_LIMIT)\n",
    "\n",
    "    # 2. Prepare sentences (tokenize + add 3 start tokens + end token)\n",
    "    prepared = prepare_sentences(sentences_raw)\n",
    "\n",
    "    # 3. Build n-gram models on full prepared dataset\n",
    "    counts, total_tokens = build_ngram_models(prepared)\n",
    "    vocab_size = len(counts['unigram'])  # includes <s> and </s>\n",
    "    print(f\"Vocab size: {vocab_size}, total tokens: {total_tokens}\")\n",
    "\n",
    "    # 4. Pre-compute follower maps for token-type smoothing\n",
    "    print(\"Computing follower counts for token-type smoothing...\")\n",
    "    follower_maps = {\n",
    "        'bigram': compute_follower_counts(counts['bigram']),\n",
    "        'trigram': compute_follower_counts(counts['trigram']),\n",
    "        'quadrigram': compute_follower_counts(counts['quadrigram'])\n",
    "    }\n",
    "    print(\"Follower maps ready.\")\n",
    "\n",
    "    # 5. Select 1000 random sentences (or fewer if corpus smaller) for evaluation\n",
    "    N_TEST = 1000\n",
    "    if len(prepared) >= N_TEST:\n",
    "        test_sentences = random.sample(prepared, N_TEST)\n",
    "    else:\n",
    "        test_sentences = prepared\n",
    "        print(f\"Warning: only {len(prepared)} sentences available for testing.\")\n",
    "\n",
    "    print(f\"Selected {len(test_sentences)} sentences for evaluation.\")\n",
    "\n",
    "    # 6. Evaluate sentences under different models & smoothings and store results\n",
    "    results = []\n",
    "    models = ['unigram', 'bigram', 'trigram', 'quadrigram']\n",
    "    k_for_add_k = 0.5  # Add-K example\n",
    "    ks_to_test = [1.0, k_for_add_k]  # add-1 (laplace) and add-k\n",
    "    # token-type smoothing will use k=1 by default here\n",
    "    params = (counts, total_tokens, vocab_size, follower_maps)\n",
    "\n",
    "    # iterate and compute log probs\n",
    "    print(\"Evaluating...\")\n",
    "    for idx, sent in enumerate(test_sentences, 1):\n",
    "        # compact sentence string for display (but keep tokens for scoring)\n",
    "        sent_str = ' '.join(sent)\n",
    "        if len(sent_str) > 120:\n",
    "            sent_brief = sent_str[:117] + '...'\n",
    "        else:\n",
    "            sent_brief = sent_str\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processed {idx}/{len(test_sentences)}\")\n",
    "\n",
    "        for model in models:\n",
    "            # Unigram only needs add-k variants (token-type not meaningful for unigram)\n",
    "            if model == 'unigram':\n",
    "                for k in ks_to_test:\n",
    "                    logp = calculate_sentence_log_prob(sent, 'unigram', 'add_k', k, params)\n",
    "                    results.append({\n",
    "                        'Sentence_ID': idx,\n",
    "                        'Sentence': sent_brief,\n",
    "                        'Model': f'unigram_add_{k}',\n",
    "                        'LogProb': logp\n",
    "                    })\n",
    "                continue\n",
    "\n",
    "            # For bigram/trigram/quadrigram: add-1, add-k, token-type(k=1)\n",
    "            # Add-1\n",
    "            logp_a1 = calculate_sentence_log_prob(sent, model, 'add_k', 1.0, params)\n",
    "            results.append({'Sentence_ID': idx, 'Sentence': sent_brief, 'Model': f'{model}_add_1', 'LogProb': logp_a1})\n",
    "            # Add-k\n",
    "            logp_ak = calculate_sentence_log_prob(sent, model, 'add_k', k_for_add_k, params)\n",
    "            results.append({'Sentence_ID': idx, 'Sentence': sent_brief, 'Model': f'{model}_add_{k_for_add_k}', 'LogProb': logp_ak})\n",
    "            # Token-type smoothing (k=1)\n",
    "            logp_tt = calculate_sentence_log_prob(sent, model, 'token_type', 1.0, params)\n",
    "            results.append({'Sentence_ID': idx, 'Sentence': sent_brief, 'Model': f'{model}_token_type_k1', 'LogProb': logp_tt})\n",
    "\n",
    "    # 7. Save results to CSV and show top rows\n",
    "    print(f\"Saving {len(results)} evaluation rows to {RESULTS_CSV} ...\")\n",
    "    # ensure output dir exists\n",
    "    out_dir = os.path.dirname(os.path.abspath(RESULTS_CSV))\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(RESULTS_CSV, index=False)\n",
    "    print(df_results.head(20).to_string(index=False))\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
